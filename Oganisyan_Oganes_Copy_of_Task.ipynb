{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bierooed/phrase-search-algorithm/blob/master/Oganisyan_Oganes_Copy_of_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and unzip the English texts collection, which contains over 100,000 documents. The documents will be saved in the /content/enwiki directory"
      ],
      "metadata": {
        "id": "VPsvgTKmIo9h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rJuO5L6YxZOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDSsSh0BU8iS"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/uc?id=1wzj7AXu_UmwrMc5Tl1_acMKbYdKWaXMC\n",
        "!unzip /content/enwiki.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section A\n",
        "\n",
        "Simple code for searching phrases"
      ],
      "metadata": {
        "id": "Uzz1ujuDw9vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code block prepares documents for further searching\n",
        "\n",
        "from typing import List, Dict\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def read_text_fragments(file_path: str) -> List[str]:\n",
        "    with open(file_path, \"r\") as f:\n",
        "        fragments = f.readlines()\n",
        "    # Remove leading and trailing spaces and new line characters before returning text fragments\n",
        "    return [fragment.strip() for fragment in fragments if fragment.strip()]\n",
        "\n",
        "def extract_and_combine_text_fragments(folder_path: str) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    This function iterates over all files in the folder and saves their fragments in a dictionary,\n",
        "    Returns:\n",
        "        A dictionary, where the dictionary key is the file name, and its value is a list of all fragments in that file.\n",
        "    Example:\n",
        "        texts_fragments_dict = {\n",
        "              \"file_1.txt\": [\"this is an example of some fragment\", \"this is an example of another fragment\"],\n",
        "              \"file_2.txt\": [\"this is another example\"],\n",
        "              ...\n",
        "        }\n",
        "    \"\"\"\n",
        "\n",
        "    texts_fragments_dict = {}\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        # Extract text file fragments\n",
        "        file_fragments = read_text_fragments(file_path)\n",
        "        # Add file name and its value to texts_fragments_dict\n",
        "        texts_fragments_dict[file_name] = file_fragments\n",
        "    return texts_fragments_dict\n",
        "\n",
        "# Extract all documents fragments from the specified folder\n",
        "folder_path = \"/content/enwiki\"\n",
        "texts_fragments_dict = extract_and_combine_text_fragments(folder_path=folder_path)"
      ],
      "metadata": {
        "id": "nNiQcWYG2qeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that searches similar phrases\n",
        "def search_similar_phrases(phrase: str, texts_fragments: Dict[str, List[str]]) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    For the given phrase, search for similar phrases in the texts_fragments dict.\n",
        "    Returns:\n",
        "        A list containing information about in which file the phrase is written and the corresponding fragment.\n",
        "        If nothing is found, it returns an empty list.\n",
        "    Example:\n",
        "        search_results = [\n",
        "            [\"file_1.txt\", \"this is an example of some fragment\"],\n",
        "            [\"file_1.txt\", \"this is an example of another fragment\"],\n",
        "            [\"file_2.txt\", \"this is another example\"]\n",
        "            ...\n",
        "        ]\n",
        "    \"\"\"\n",
        "    search_results = []\n",
        "    # Iterate over all files\n",
        "    for file_name in texts_fragments:\n",
        "        # Iterate over file fragments\n",
        "        for fragment in texts_fragments[file_name]:\n",
        "            # Check if the phrase is in our fragment, then save it in the search_results list\n",
        "            if phrase in fragment:\n",
        "                search_results.append([file_name, fragment])\n",
        "\n",
        "    return search_results"
      ],
      "metadata": {
        "id": "0ym1bWUUEdqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of searching\n",
        "phrase = \"Berisha is elected chairman\"\n",
        "search_results = search_similar_phrases(phrase, texts_fragments_dict)\n",
        "print(search_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-25z27aGHyZ",
        "outputId": "4dc181c7-0921-476b-80c8-4c3fe57ddf25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['17919119.txt', 'Berisha is elected chairman of the Democratic Party.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This algorithm works slowly; if we have, for example, a million texts, it can take minutes to return an answer.\n",
        "\n",
        "Besides this, the proposed method only finds identical texts."
      ],
      "metadata": {
        "id": "kieOz_A5GaqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. If we delete some of the middle words, we will receive an empty result\n",
        "phrase = \"Berisha elected chairman\"\n",
        "search_results = search_similar_phrases(phrase, texts_fragments_dict)\n",
        "print(search_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT3w5MmkGZmV",
        "outputId": "0a839f48-4f62-47c0-c4e4-6e9041c7338d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Or if we delete some word ending\n",
        "phrase = \"Berisha is elect chairman\"\n",
        "search_results = search_similar_phrases(phrase, texts_fragments_dict)\n",
        "print(search_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACapfPnyGuGe",
        "outputId": "3393954e-0491-4371-9b5e-d51ef7296b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Or if we make words lowercase\n",
        "phrase = \"berisha is elected chairman\"\n",
        "search_results = search_similar_phrases(phrase, texts_fragments_dict)\n",
        "print(search_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LRY7VotGuJX",
        "outputId": "65a23582-8aaa-4e93-8fb5-6777289336d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Or if we change some similar meaning words, \"elected\" -> \"appointed\"\n",
        "phrase = \"Berisha is appointed chairman\"\n",
        "search_results = search_similar_phrases(phrase, texts_fragments_dict)\n",
        "print(search_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnHSNgiyGuMU",
        "outputId": "caac1c4e-70b1-4e7b-c6a4-da3c6344fbeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section B\n",
        "\n",
        "Research and implement a search algorithm that overcomes these limitations and performs efficiently and rapidly on a vast collection of documents. Additionally, you need to explore other potential problems that might occur during the search process and propose solutions to avoid them.\n"
      ],
      "metadata": {
        "id": "GK6qC8znMp0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "import os\n",
        "\n",
        "def read_text_fragments(file_path: str) -> List[str]:\n",
        "    with open(file_path, 'r') as f:\n",
        "        fragments = f.readlines()\n",
        "        return [fragment.strip() for fragment in fragments if fragment.strip()]\n",
        "\n",
        "\n",
        "def extract_and_combine_text_fragments(folder_path: str) -> Dict[str, List[str]]:\n",
        "    texts_fragments_dict = {}\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        if os.path.isfile(file_path):\n",
        "            fragments = read_text_fragments(file_path)\n",
        "            texts_fragments_dict[file_name] = fragments\n",
        "    return texts_fragments_dict\n",
        "\n",
        "\n",
        "class InvertedIndex:\n",
        "    def __init__(self):\n",
        "        self.index = {}\n",
        "\n",
        "    def add_document(self, doc_id, fragment):\n",
        "        translation_table = str.maketrans(\"\", \"\", \".,()\")  # Таблица для удаления символов\n",
        "        cleaned_fragment = fragment.translate(translation_table)\n",
        "        tokens = cleaned_fragment.lower().split()\n",
        "        for token in tokens:\n",
        "            if token not in self.index:\n",
        "                self.index[token] = []\n",
        "            self.index[token].append(doc_id)\n",
        "\n",
        "    def search(self, query):\n",
        "        query_tokens = query.lower().split()\n",
        "        doc_matches = {}\n",
        "        for token in query_tokens:\n",
        "            if token in self.index:\n",
        "                for doc_id in self.index[token]:\n",
        "                    if doc_id not in doc_matches:\n",
        "                        doc_matches[doc_id] = set()\n",
        "                    doc_matches[doc_id].add(token)\n",
        "\n",
        "        result = []\n",
        "        for doc_id, matched_tokens in doc_matches.items():\n",
        "            if len(matched_tokens) == len(query_tokens):\n",
        "                result.append(doc_id)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_fragment(self, doc_id, fragment_idx):\n",
        "        if doc_id in documents and 0 <= fragment_idx < len(documents[doc_id]):\n",
        "            return documents[doc_id][fragment_idx]\n",
        "        return None\n",
        "\n",
        "\n",
        "documents = extract_and_combine_text_fragments('./text_files')\n",
        "\n",
        "inverted_index = InvertedIndex()\n",
        "\n",
        "for doc_id, fragments in documents.items():\n",
        "    for fragment in fragments:\n",
        "        inverted_index.add_document(doc_id, fragment)\n",
        "\n",
        "query = \"Berisha is elected chairman of the Democratic Party\"\n",
        "print(len(query.split()))\n",
        "search_results = inverted_index.search(query)\n",
        "print(search_results, ' ---')\n",
        "for doc_id in search_results:\n",
        "    matching_fragments = [fragment for fragment in documents[doc_id] if sum(1 for token in query.lower().split() if token in fragment.lower()) >= len(query.split())]\n",
        "    for fragment in matching_fragments:\n",
        "        print(f\"Found in document: {doc_id}, fragment: {fragment}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SmGPNWL4VVz4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}